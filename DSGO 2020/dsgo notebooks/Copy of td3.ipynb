{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of td3.ipynb","provenance":[{"file_id":"14Vx6wUOqHi_K5Sczg0eU9axnJ8KbXyhX","timestamp":1592665034841},{"file_id":"1um2nqgA4ZFiQ4HMdBvUY9KmoZo0VUE5e","timestamp":1556628348895}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WXu1r8qvSzWf","colab_type":"text"},"source":["# TD3\n","Workshop for DSGO 2020 event (20/06/2020)"]},{"cell_type":"markdown","metadata":{"id":"YRzQUhuUTc0J","colab_type":"text"},"source":["## Installing the packages"]},{"cell_type":"code","metadata":{"id":"HAHMB0Ze8fU0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1592687143870,"user_tz":-120,"elapsed":17697,"user":{"displayName":"Giulia C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiiG5I96a5FcE331UswvgIGOqp_10Ljk1qHdQma=s64","userId":"04327425902310475783"}},"outputId":"5d01d3b0-21e5-4abe-f118-a5d873bf07e9"},"source":["!pip install pybullet"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pybullet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/60/dcdbe419dc7742058f82c58dba0eb67eae48ee0fabbc2f8004b81be61ad3/pybullet-2.8.2-cp36-cp36m-manylinux1_x86_64.whl (95.2MB)\n","\u001b[K     |████████████████████████████████| 95.2MB 60kB/s \n","\u001b[?25hInstalling collected packages: pybullet\n","Successfully installed pybullet-2.8.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xjm2onHdT-Av","colab_type":"text"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"Ikr2p0Js8iB4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592687166096,"user_tz":-120,"elapsed":3269,"user":{"displayName":"Giulia C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiiG5I96a5FcE331UswvgIGOqp_10Ljk1qHdQma=s64","userId":"04327425902310475783"}}},"source":["import os\n","import time\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pybullet_envs\n","import gym # reinf learn\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gym import wrappers\n","from torch.autograd import Variable # gradient descent\n","from collections import deque"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y2nGdtlKVydr","colab_type":"text"},"source":["## Step 1: We initialize the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"u5rW0IDB8nTO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592687374720,"user_tz":-120,"elapsed":599,"user":{"displayName":"Giulia C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiiG5I96a5FcE331UswvgIGOqp_10Ljk1qHdQma=s64","userId":"04327425902310475783"}}},"source":["class ReplayBuffer(object):\n","\n","  def __init__(self, max_size=1e6):\n","    self.storage = []\n","    self.max_size = max_size\n","    self.ptr = 0\n","\n","  def add(self, transition):\n","    if len(self.storage) == self.max_size:\n","      self.storage[int(self.ptr)] = transition\n","      self.ptr = (self.ptr + 1) % self.max_size\n","    else:\n","      self.storage.append(transition)\n","\n","  def sample(self, batch_size):\n","    ind = np.random.randint(0, len(self.storage), size=batch_size)\n","    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n","    for i in ind: \n","      state, next_state, action, reward, done = self.storage[i]\n","      batch_states.append(np.array(state, copy=False))\n","      batch_next_states.append(np.array(next_state, copy=False))\n","      batch_actions.append(np.array(action, copy=False))\n","      batch_rewards.append(np.array(reward, copy=False))\n","      batch_dones.append(np.array(done, copy=False))\n","    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jb7TTaHxWbQD","colab_type":"text"},"source":["## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"]},{"cell_type":"code","metadata":{"id":"4CeRW4D79HL0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592687444518,"user_tz":-120,"elapsed":572,"user":{"displayName":"Giulia C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiiG5I96a5FcE331UswvgIGOqp_10Ljk1qHdQma=s64","userId":"04327425902310475783"}}},"source":["class Actor(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return x"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRDDce8FXef7","colab_type":"text"},"source":["## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"]},{"cell_type":"code","metadata":{"id":"OCee7gwR9Jrs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592687594793,"user_tz":-120,"elapsed":613,"user":{"displayName":"Giulia C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiiG5I96a5FcE331UswvgIGOqp_10Ljk1qHdQma=s64","userId":"04327425902310475783"}}},"source":["class Critic(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzIDuONodenW","colab_type":"text"},"source":["## Steps 4 to 15: Training Process"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zzd0H1xukdKe","colab":{},"executionInfo":{"status":"ok","timestamp":1592687857124,"user_tz":-120,"elapsed":714,"user":{"displayName":"Giulia C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiiG5I96a5FcE331UswvgIGOqp_10Ljk1qHdQma=s64","userId":"04327425902310475783"}}},"source":["# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","    \n","    for it in range(iterations):\n","      \n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","      \n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","      \n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","      \n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","      \n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","      \n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","      \n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","      \n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","      \n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","      \n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","        \n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","        \n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","  \n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","  \n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ka-ZRtQvjBex","colab_type":"text"},"source":["## We make a function that evaluates the policy by calculating its average reward over 10 episodes"]},{"cell_type":"code","metadata":{"id":"qabqiYdp9wDM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592688125709,"user_tz":-120,"elapsed":562,"user":{"displayName":"Giulia C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiiG5I96a5FcE331UswvgIGOqp_10Ljk1qHdQma=s64","userId":"04327425902310475783"}}},"source":["def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGuKmH_ijf7U","colab_type":"text"},"source":["## We set the parameters"]},{"cell_type":"code","metadata":{"id":"HFj6wbAo97lk","colab_type":"code","colab":{}},"source":["env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n","seed = 0 # Random seed number\n","start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n","eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n","max_timesteps = 5e5 # Total number of iterations/timesteps\n","save_models = True # Boolean checker whether or not to save the pre-trained model\n","expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n","batch_size = 100 # Size of the batch\n","discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n","tau = 0.005 # Target network update rate\n","policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n","noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n","policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hjwf2HCol3XP","colab_type":"text"},"source":["## We create a file name for the two saved models: the Actor and Critic models"]},{"cell_type":"code","metadata":{"id":"1fyH8N5z-o3o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1592478623731,"user_tz":-120,"elapsed":22660,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"outputId":"0fca8aa1-95b3-426a-9d33-884dbb42731d"},"source":["file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------------\n","Settings: TD3_AntBulletEnv-v0_0\n","---------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kop-C96Aml8O","colab_type":"text"},"source":["## We create a folder inside which will be saved the trained models"]},{"cell_type":"code","metadata":{"id":"Src07lvY-zXb","colab_type":"code","colab":{}},"source":["if not os.path.exists(\"./results\"):\n","  os.makedirs(\"./results\")\n","if save_models and not os.path.exists(\"./pytorch_models\"):\n","  os.makedirs(\"./pytorch_models\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEAzOd47mv1Z","colab_type":"text"},"source":["## We create the PyBullet environment"]},{"cell_type":"code","metadata":{"id":"CyQXJUIs-6BV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1592478623732,"user_tz":-120,"elapsed":22651,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"outputId":"e76ed691-f54a-4255-e1ad-78843eaf2c3e"},"source":["env = gym.make(env_name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5YdPG4HXnNsh","colab_type":"text"},"source":["## We set seeds and we get the necessary information on the states and actions in the chosen environment"]},{"cell_type":"code","metadata":{"id":"Z3RufYec_ADj","colab_type":"code","colab":{}},"source":["env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWEgDAQxnbem","colab_type":"text"},"source":["## We create the policy network (the Actor model)"]},{"cell_type":"code","metadata":{"id":"wTVvG7F8_EWg","colab_type":"code","colab":{}},"source":["policy = TD3(state_dim, action_dim, max_action)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZI60VN2Unklh","colab_type":"text"},"source":["## We create the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"sd-ZsdXR_LgV","colab_type":"code","colab":{}},"source":["replay_buffer = ReplayBuffer()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYOpCyiDnw7s","colab_type":"text"},"source":["## We define a list where all the evaluation results over 10 episodes are stored"]},{"cell_type":"code","metadata":{"id":"dhC_5XJ__Orp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1592478624324,"user_tz":-120,"elapsed":23223,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"outputId":"d093451e-0241-4888-86e2-2d435da88080"},"source":["evaluations = [evaluate_policy(policy)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------------\n","Average Reward over the Evaluation Step: 9.804960\n","---------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xm-4b3p6rglE","colab_type":"text"},"source":["## We create a new folder directory in which the final results (videos of the agent) will be populated"]},{"cell_type":"code","metadata":{"id":"MTL9uMd0ru03","colab_type":"code","colab":{}},"source":["def mkdir(base, name):\n","    path = os.path.join(base, name)\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    return path\n","work_dir = mkdir('exp', 'brs')\n","monitor_dir = mkdir(work_dir, 'monitor')\n","max_episode_steps = env._max_episode_steps\n","save_env_vid = False\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31n5eb03p-Fm","colab_type":"text"},"source":["## We initialize the variables"]},{"cell_type":"code","metadata":{"id":"1vN5EvxK_QhT","colab_type":"code","colab":{}},"source":["total_timesteps = 0\n","timesteps_since_eval = 0\n","episode_num = 0\n","done = True\n","t0 = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9gsjvtPqLgT","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"y_ouY4NH_Y0I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592495601810,"user_tz":-120,"elapsed":17000694,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"outputId":"73528520-fe0f-4d2a-d2fa-e2b9ac5d8d7e"},"source":["# We start the main loop over 500,000 timesteps\n","while total_timesteps < max_timesteps:\n","  \n","  # If the episode is done\n","  if done:\n","\n","    # If we are not at the very beginning, we start the training process of the model\n","    if total_timesteps != 0:\n","      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n","      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n","\n","    # We evaluate the episode and we save the policy\n","    if timesteps_since_eval >= eval_freq:\n","      timesteps_since_eval %= eval_freq\n","      evaluations.append(evaluate_policy(policy))\n","      policy.save(file_name, directory=\"./pytorch_models\")\n","      np.save(\"./results/%s\" % (file_name), evaluations)\n","    \n","    # When the training step is done, we reset the state of the environment\n","    obs = env.reset()\n","    \n","    # Set the Done to False\n","    done = False\n","    \n","    # Set rewards and episode timesteps to zero\n","    episode_reward = 0\n","    episode_timesteps = 0\n","    episode_num += 1\n","  \n","  # Before 10000 timesteps, we play random actions\n","  if total_timesteps < start_timesteps:\n","    action = env.action_space.sample()\n","  else: # After 10000 timesteps, we switch to the model\n","    action = policy.select_action(np.array(obs))\n","    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n","    if expl_noise != 0:\n","      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n","  \n","  # The agent performs the action in the environment, then reaches the next state and receives the reward\n","  new_obs, reward, done, _ = env.step(action)\n","  \n","  # We check if the episode is done\n","  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n","  \n","  # We increase the total reward\n","  episode_reward += reward\n","  \n","  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n","  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n","\n","  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n","  obs = new_obs\n","  episode_timesteps += 1\n","  total_timesteps += 1\n","  timesteps_since_eval += 1\n","\n","# We add the last policy evaluation to our list of evaluations and we save our model\n","evaluations.append(evaluate_policy(policy))\n","if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n","np.save(\"./results/%s\" % (file_name), evaluations)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Timesteps: 1000 Episode Num: 1 Reward: 505.8829667458676\n","Total Timesteps: 2000 Episode Num: 2 Reward: 509.61361559211804\n","Total Timesteps: 3000 Episode Num: 3 Reward: 531.5491673901299\n","Total Timesteps: 4000 Episode Num: 4 Reward: 525.7907657999363\n","Total Timesteps: 5000 Episode Num: 5 Reward: 467.06361659154214\n","---------------------------------------\n","Average Reward over the Evaluation Step: 141.380491\n","---------------------------------------\n","Total Timesteps: 5712 Episode Num: 6 Reward: 350.68316294745483\n","Total Timesteps: 6566 Episode Num: 7 Reward: 406.4893142430121\n","Total Timesteps: 7142 Episode Num: 8 Reward: 319.51867756021994\n","Total Timesteps: 8142 Episode Num: 9 Reward: 391.2358873571006\n","Total Timesteps: 8433 Episode Num: 10 Reward: 172.6046472623973\n","Total Timesteps: 9433 Episode Num: 11 Reward: 486.06354773074867\n","Total Timesteps: 10433 Episode Num: 12 Reward: 430.1867026546684\n","---------------------------------------\n","Average Reward over the Evaluation Step: 129.857823\n","---------------------------------------\n","Total Timesteps: 11433 Episode Num: 13 Reward: 141.21880172989117\n","Total Timesteps: 12433 Episode Num: 14 Reward: 149.32265755077762\n","Total Timesteps: 13433 Episode Num: 15 Reward: 101.24943487932225\n","Total Timesteps: 14433 Episode Num: 16 Reward: 97.03743350019424\n","Total Timesteps: 15433 Episode Num: 17 Reward: 274.04177336366115\n","---------------------------------------\n","Average Reward over the Evaluation Step: 302.920259\n","---------------------------------------\n","Total Timesteps: 16433 Episode Num: 18 Reward: 380.2019631145244\n","Total Timesteps: 17433 Episode Num: 19 Reward: 286.3700312693244\n","Total Timesteps: 18433 Episode Num: 20 Reward: 243.52931829071875\n","Total Timesteps: 19433 Episode Num: 21 Reward: 501.5502819770752\n","Total Timesteps: 20433 Episode Num: 22 Reward: 545.4419056835196\n","---------------------------------------\n","Average Reward over the Evaluation Step: 251.423542\n","---------------------------------------\n","Total Timesteps: 21433 Episode Num: 23 Reward: 147.13552356652187\n","Total Timesteps: 22433 Episode Num: 24 Reward: 614.0852749799906\n","Total Timesteps: 22588 Episode Num: 25 Reward: 76.6315771355359\n","Total Timesteps: 23588 Episode Num: 26 Reward: 464.51745102484756\n","Total Timesteps: 24588 Episode Num: 27 Reward: 543.6904053295285\n","Total Timesteps: 25588 Episode Num: 28 Reward: 576.5708501110043\n","---------------------------------------\n","Average Reward over the Evaluation Step: 355.742729\n","---------------------------------------\n","Total Timesteps: 26588 Episode Num: 29 Reward: 376.15200092152054\n","Total Timesteps: 27588 Episode Num: 30 Reward: 669.2259299618288\n","Total Timesteps: 28588 Episode Num: 31 Reward: 405.4298903084841\n","Total Timesteps: 29588 Episode Num: 32 Reward: 400.03510314524675\n","Total Timesteps: 30588 Episode Num: 33 Reward: 332.8376290678864\n","---------------------------------------\n","Average Reward over the Evaluation Step: 318.806520\n","---------------------------------------\n","Total Timesteps: 31588 Episode Num: 34 Reward: 332.5068474632916\n","Total Timesteps: 32387 Episode Num: 35 Reward: 384.6731076819413\n","Total Timesteps: 32407 Episode Num: 36 Reward: 2.9838419426773863\n","Total Timesteps: 32427 Episode Num: 37 Reward: 2.6158963260940298\n","Total Timesteps: 32455 Episode Num: 38 Reward: 5.102824148824682\n","Total Timesteps: 32476 Episode Num: 39 Reward: 2.250431968614497\n","Total Timesteps: 33476 Episode Num: 40 Reward: 422.6608749638219\n","Total Timesteps: 33496 Episode Num: 41 Reward: 0.12601482209832104\n","Total Timesteps: 33516 Episode Num: 42 Reward: 0.15326733952826288\n","Total Timesteps: 33536 Episode Num: 43 Reward: 0.8775525278434526\n","Total Timesteps: 34536 Episode Num: 44 Reward: 521.8241451761896\n","Total Timesteps: 35536 Episode Num: 45 Reward: 373.8014632750166\n","---------------------------------------\n","Average Reward over the Evaluation Step: 284.901180\n","---------------------------------------\n","Total Timesteps: 36536 Episode Num: 46 Reward: 257.3980295325464\n","Total Timesteps: 37079 Episode Num: 47 Reward: 188.6004679011765\n","Total Timesteps: 38079 Episode Num: 48 Reward: 687.1341117372287\n","Total Timesteps: 39079 Episode Num: 49 Reward: 485.597465244831\n","Total Timesteps: 39109 Episode Num: 50 Reward: 3.987014841952365\n","Total Timesteps: 39134 Episode Num: 51 Reward: 7.519368580554739\n","Total Timesteps: 39609 Episode Num: 52 Reward: 188.79819453420365\n","Total Timesteps: 40609 Episode Num: 53 Reward: 550.7611010357815\n","---------------------------------------\n","Average Reward over the Evaluation Step: 520.942953\n","---------------------------------------\n","Total Timesteps: 41609 Episode Num: 54 Reward: 455.13310194262385\n","Total Timesteps: 42609 Episode Num: 55 Reward: 324.5147982128247\n","Total Timesteps: 43609 Episode Num: 56 Reward: 599.0893857738381\n","Total Timesteps: 44609 Episode Num: 57 Reward: 643.0673733944643\n","Total Timesteps: 45609 Episode Num: 58 Reward: 367.01436978835113\n","---------------------------------------\n","Average Reward over the Evaluation Step: 466.077749\n","---------------------------------------\n","Total Timesteps: 46609 Episode Num: 59 Reward: 475.7942299339026\n","Total Timesteps: 47453 Episode Num: 60 Reward: 267.6782779950549\n","Total Timesteps: 48453 Episode Num: 61 Reward: 529.3166893264736\n","Total Timesteps: 49453 Episode Num: 62 Reward: 383.0930039215027\n","Total Timesteps: 50453 Episode Num: 63 Reward: 336.16720981908037\n","---------------------------------------\n","Average Reward over the Evaluation Step: 428.881067\n","---------------------------------------\n","Total Timesteps: 51453 Episode Num: 64 Reward: 500.2544640574439\n","Total Timesteps: 52453 Episode Num: 65 Reward: 321.2599015868787\n","Total Timesteps: 53453 Episode Num: 66 Reward: 577.0935537503877\n","Total Timesteps: 54453 Episode Num: 67 Reward: 305.61544203524176\n","Total Timesteps: 55453 Episode Num: 68 Reward: 378.03618400715146\n","---------------------------------------\n","Average Reward over the Evaluation Step: 604.870842\n","---------------------------------------\n","Total Timesteps: 56453 Episode Num: 69 Reward: 550.4624978315348\n","Total Timesteps: 57453 Episode Num: 70 Reward: 334.7669111869394\n","Total Timesteps: 58453 Episode Num: 71 Reward: 424.88488691336147\n","Total Timesteps: 59182 Episode Num: 72 Reward: 447.2097481443802\n","Total Timesteps: 60182 Episode Num: 73 Reward: 647.2518193589698\n","---------------------------------------\n","Average Reward over the Evaluation Step: 434.926088\n","---------------------------------------\n","Total Timesteps: 61182 Episode Num: 74 Reward: 312.1221167445441\n","Total Timesteps: 62067 Episode Num: 75 Reward: 310.84826734027405\n","Total Timesteps: 63067 Episode Num: 76 Reward: 270.4055531910645\n","Total Timesteps: 64067 Episode Num: 77 Reward: 429.8574633176638\n","Total Timesteps: 64393 Episode Num: 78 Reward: 134.56596218303136\n","Total Timesteps: 64655 Episode Num: 79 Reward: 75.7444639804016\n","Total Timesteps: 64745 Episode Num: 80 Reward: 25.955192038895472\n","Total Timesteps: 65745 Episode Num: 81 Reward: 427.7916957713388\n","---------------------------------------\n","Average Reward over the Evaluation Step: 292.464235\n","---------------------------------------\n","Total Timesteps: 66002 Episode Num: 82 Reward: 141.5867357105747\n","Total Timesteps: 67002 Episode Num: 83 Reward: 618.2437804632949\n","Total Timesteps: 68002 Episode Num: 84 Reward: 522.8349378848008\n","Total Timesteps: 69002 Episode Num: 85 Reward: 432.41738548592554\n","Total Timesteps: 70002 Episode Num: 86 Reward: 711.1805856828277\n","---------------------------------------\n","Average Reward over the Evaluation Step: 536.343789\n","---------------------------------------\n","Total Timesteps: 71002 Episode Num: 87 Reward: 623.6413661172336\n","Total Timesteps: 72002 Episode Num: 88 Reward: 324.7642770561167\n","Total Timesteps: 73002 Episode Num: 89 Reward: 338.1784909639158\n","Total Timesteps: 74002 Episode Num: 90 Reward: 407.88869305311374\n","Total Timesteps: 75002 Episode Num: 91 Reward: 583.7093597199051\n","---------------------------------------\n","Average Reward over the Evaluation Step: 232.030436\n","---------------------------------------\n","Total Timesteps: 76002 Episode Num: 92 Reward: 146.0046988567217\n","Total Timesteps: 77002 Episode Num: 93 Reward: 252.1209117825216\n","Total Timesteps: 78002 Episode Num: 94 Reward: 375.108736997137\n","Total Timesteps: 79002 Episode Num: 95 Reward: 466.2739019894914\n","Total Timesteps: 80002 Episode Num: 96 Reward: 389.09252558194487\n","---------------------------------------\n","Average Reward over the Evaluation Step: 474.841189\n","---------------------------------------\n","Total Timesteps: 81002 Episode Num: 97 Reward: 454.03495542121476\n","Total Timesteps: 82002 Episode Num: 98 Reward: 498.2186726679827\n","Total Timesteps: 83002 Episode Num: 99 Reward: 470.1272663934289\n","Total Timesteps: 84002 Episode Num: 100 Reward: 406.00699959872054\n","Total Timesteps: 85002 Episode Num: 101 Reward: 364.7449838350884\n","---------------------------------------\n","Average Reward over the Evaluation Step: 589.373538\n","---------------------------------------\n","Total Timesteps: 86002 Episode Num: 102 Reward: 501.9950517948328\n","Total Timesteps: 87002 Episode Num: 103 Reward: 658.2536424517875\n","Total Timesteps: 88002 Episode Num: 104 Reward: 529.0706905877995\n","Total Timesteps: 89002 Episode Num: 105 Reward: 646.0346170483581\n","Total Timesteps: 90002 Episode Num: 106 Reward: 315.2317356490482\n","---------------------------------------\n","Average Reward over the Evaluation Step: 492.172340\n","---------------------------------------\n","Total Timesteps: 91002 Episode Num: 107 Reward: 564.9912866237354\n","Total Timesteps: 91238 Episode Num: 108 Reward: 120.57862690692183\n","Total Timesteps: 92238 Episode Num: 109 Reward: 647.1388228907338\n","Total Timesteps: 93238 Episode Num: 110 Reward: 756.3220793649967\n","Total Timesteps: 94238 Episode Num: 111 Reward: 368.16997658268747\n","Total Timesteps: 95238 Episode Num: 112 Reward: 437.021443757485\n","---------------------------------------\n","Average Reward over the Evaluation Step: 411.278676\n","---------------------------------------\n","Total Timesteps: 96238 Episode Num: 113 Reward: 444.20091029584745\n","Total Timesteps: 97238 Episode Num: 114 Reward: 437.8383334103301\n","Total Timesteps: 98238 Episode Num: 115 Reward: 702.5469684230544\n","Total Timesteps: 99238 Episode Num: 116 Reward: 621.7011837332708\n","Total Timesteps: 100238 Episode Num: 117 Reward: 722.2143643908067\n","---------------------------------------\n","Average Reward over the Evaluation Step: 534.708721\n","---------------------------------------\n","Total Timesteps: 101238 Episode Num: 118 Reward: 579.9643899529204\n","Total Timesteps: 102238 Episode Num: 119 Reward: 561.7204335033157\n","Total Timesteps: 103238 Episode Num: 120 Reward: 730.2011888359773\n","Total Timesteps: 104238 Episode Num: 121 Reward: 431.644666724049\n","Total Timesteps: 105238 Episode Num: 122 Reward: 482.9751293039017\n","---------------------------------------\n","Average Reward over the Evaluation Step: 488.650884\n","---------------------------------------\n","Total Timesteps: 106238 Episode Num: 123 Reward: 684.9611986708121\n","Total Timesteps: 107238 Episode Num: 124 Reward: 346.53783641041974\n","Total Timesteps: 108238 Episode Num: 125 Reward: 477.4593146351488\n","Total Timesteps: 109238 Episode Num: 126 Reward: 637.2756073471168\n","Total Timesteps: 110238 Episode Num: 127 Reward: 751.6964857219169\n","---------------------------------------\n","Average Reward over the Evaluation Step: 644.218386\n","---------------------------------------\n","Total Timesteps: 111238 Episode Num: 128 Reward: 618.733572959297\n","Total Timesteps: 112238 Episode Num: 129 Reward: 574.5922953362516\n","Total Timesteps: 113238 Episode Num: 130 Reward: 549.2674244501377\n","Total Timesteps: 114238 Episode Num: 131 Reward: 606.3618189204795\n","Total Timesteps: 115238 Episode Num: 132 Reward: 597.1472331680807\n","---------------------------------------\n","Average Reward over the Evaluation Step: 564.206700\n","---------------------------------------\n","Total Timesteps: 116238 Episode Num: 133 Reward: 624.6712464134303\n","Total Timesteps: 117238 Episode Num: 134 Reward: 566.7271101543205\n","Total Timesteps: 118238 Episode Num: 135 Reward: 451.04638654978083\n","Total Timesteps: 119238 Episode Num: 136 Reward: 322.9257037667744\n","Total Timesteps: 120238 Episode Num: 137 Reward: 562.1607705938494\n","---------------------------------------\n","Average Reward over the Evaluation Step: 486.937952\n","---------------------------------------\n","Total Timesteps: 121238 Episode Num: 138 Reward: 562.2468053400395\n","Total Timesteps: 122238 Episode Num: 139 Reward: 606.8556953069526\n","Total Timesteps: 123238 Episode Num: 140 Reward: 453.436795429961\n","Total Timesteps: 124238 Episode Num: 141 Reward: 519.30568321124\n","Total Timesteps: 125238 Episode Num: 142 Reward: 439.11065899543337\n","---------------------------------------\n","Average Reward over the Evaluation Step: 561.226416\n","---------------------------------------\n","Total Timesteps: 126238 Episode Num: 143 Reward: 689.5542139749274\n","Total Timesteps: 127238 Episode Num: 144 Reward: 602.0163563164646\n","Total Timesteps: 128238 Episode Num: 145 Reward: 549.4494726712566\n","Total Timesteps: 129238 Episode Num: 146 Reward: 688.6289492932466\n","Total Timesteps: 130238 Episode Num: 147 Reward: 481.7758692820777\n","---------------------------------------\n","Average Reward over the Evaluation Step: 483.309083\n","---------------------------------------\n","Total Timesteps: 131238 Episode Num: 148 Reward: 417.8939105960174\n","Total Timesteps: 132238 Episode Num: 149 Reward: 311.38038467225044\n","Total Timesteps: 133238 Episode Num: 150 Reward: 257.2702281912537\n","Total Timesteps: 134238 Episode Num: 151 Reward: 454.1391055433662\n","Total Timesteps: 135238 Episode Num: 152 Reward: 376.13675427831487\n","---------------------------------------\n","Average Reward over the Evaluation Step: 631.352799\n","---------------------------------------\n","Total Timesteps: 136238 Episode Num: 153 Reward: 691.5153819746446\n","Total Timesteps: 137238 Episode Num: 154 Reward: 673.4901213804999\n","Total Timesteps: 138238 Episode Num: 155 Reward: 763.771420018321\n","Total Timesteps: 139238 Episode Num: 156 Reward: 475.866915534858\n","Total Timesteps: 140238 Episode Num: 157 Reward: 442.9622313540718\n","---------------------------------------\n","Average Reward over the Evaluation Step: 540.878888\n","---------------------------------------\n","Total Timesteps: 141238 Episode Num: 158 Reward: 388.91307709490786\n","Total Timesteps: 142238 Episode Num: 159 Reward: 627.4326170683016\n","Total Timesteps: 143238 Episode Num: 160 Reward: 642.2395723174014\n","Total Timesteps: 144238 Episode Num: 161 Reward: 505.1151896913479\n","Total Timesteps: 145238 Episode Num: 162 Reward: 672.5791950545005\n","---------------------------------------\n","Average Reward over the Evaluation Step: 668.589519\n","---------------------------------------\n","Total Timesteps: 146238 Episode Num: 163 Reward: 595.6643973013273\n","Total Timesteps: 147238 Episode Num: 164 Reward: 583.8111047498339\n","Total Timesteps: 148238 Episode Num: 165 Reward: 755.0858917599056\n","Total Timesteps: 149238 Episode Num: 166 Reward: 821.2759411374192\n","Total Timesteps: 150238 Episode Num: 167 Reward: 682.3294924600553\n","---------------------------------------\n","Average Reward over the Evaluation Step: 667.527456\n","---------------------------------------\n","Total Timesteps: 151238 Episode Num: 168 Reward: 673.3585274142888\n","Total Timesteps: 152238 Episode Num: 169 Reward: 693.7281661164708\n","Total Timesteps: 153238 Episode Num: 170 Reward: 700.4781957409725\n","Total Timesteps: 154238 Episode Num: 171 Reward: 670.875850781205\n","Total Timesteps: 155238 Episode Num: 172 Reward: 613.6947683000067\n","---------------------------------------\n","Average Reward over the Evaluation Step: 646.321415\n","---------------------------------------\n","Total Timesteps: 156238 Episode Num: 173 Reward: 511.18411398564353\n","Total Timesteps: 156985 Episode Num: 174 Reward: 538.7277254725572\n","Total Timesteps: 157985 Episode Num: 175 Reward: 757.8858723584613\n","Total Timesteps: 158985 Episode Num: 176 Reward: 795.7756572442335\n","Total Timesteps: 159985 Episode Num: 177 Reward: 746.4683433994988\n","Total Timesteps: 160985 Episode Num: 178 Reward: 692.2453170577825\n","---------------------------------------\n","Average Reward over the Evaluation Step: 687.501896\n","---------------------------------------\n","Total Timesteps: 161985 Episode Num: 179 Reward: 409.18707986072434\n","Total Timesteps: 162985 Episode Num: 180 Reward: 606.3443215201308\n","Total Timesteps: 163985 Episode Num: 181 Reward: 738.1062694135787\n","Total Timesteps: 164985 Episode Num: 182 Reward: 562.2041443274874\n","Total Timesteps: 165985 Episode Num: 183 Reward: 617.1761021956502\n","---------------------------------------\n","Average Reward over the Evaluation Step: 767.882008\n","---------------------------------------\n","Total Timesteps: 166985 Episode Num: 184 Reward: 904.1834077654112\n","Total Timesteps: 167985 Episode Num: 185 Reward: 793.9116720206803\n","Total Timesteps: 168985 Episode Num: 186 Reward: 706.3651448764307\n","Total Timesteps: 169985 Episode Num: 187 Reward: 731.851626900934\n","Total Timesteps: 170985 Episode Num: 188 Reward: 681.7362726638005\n","---------------------------------------\n","Average Reward over the Evaluation Step: 685.901297\n","---------------------------------------\n","Total Timesteps: 171985 Episode Num: 189 Reward: 930.9614698033091\n","Total Timesteps: 172985 Episode Num: 190 Reward: 800.5649272341739\n","Total Timesteps: 173985 Episode Num: 191 Reward: 769.483477686531\n","Total Timesteps: 174668 Episode Num: 192 Reward: 536.5083362158107\n","Total Timesteps: 175668 Episode Num: 193 Reward: 720.1427487650703\n","---------------------------------------\n","Average Reward over the Evaluation Step: 666.755001\n","---------------------------------------\n","Total Timesteps: 176668 Episode Num: 194 Reward: 666.4699106302525\n","Total Timesteps: 177668 Episode Num: 195 Reward: 663.8902315703252\n","Total Timesteps: 178668 Episode Num: 196 Reward: 735.2600987333665\n","Total Timesteps: 179668 Episode Num: 197 Reward: 670.4042712013725\n","Total Timesteps: 180668 Episode Num: 198 Reward: 737.4938164348481\n","---------------------------------------\n","Average Reward over the Evaluation Step: 499.277221\n","---------------------------------------\n","Total Timesteps: 181668 Episode Num: 199 Reward: 476.03231613860555\n","Total Timesteps: 182668 Episode Num: 200 Reward: 784.6498260239324\n","Total Timesteps: 183668 Episode Num: 201 Reward: 595.0328313588911\n","Total Timesteps: 184668 Episode Num: 202 Reward: 796.3594174903253\n","Total Timesteps: 185668 Episode Num: 203 Reward: 925.311799931249\n","---------------------------------------\n","Average Reward over the Evaluation Step: 837.022896\n","---------------------------------------\n","Total Timesteps: 186668 Episode Num: 204 Reward: 734.7382683527842\n","Total Timesteps: 187668 Episode Num: 205 Reward: 845.2566242682648\n","Total Timesteps: 188668 Episode Num: 206 Reward: 691.0533927878116\n","Total Timesteps: 189668 Episode Num: 207 Reward: 686.4267603067028\n","Total Timesteps: 190668 Episode Num: 208 Reward: 579.7461106698998\n","---------------------------------------\n","Average Reward over the Evaluation Step: 873.697685\n","---------------------------------------\n","Total Timesteps: 191668 Episode Num: 209 Reward: 707.7815990565273\n","Total Timesteps: 192668 Episode Num: 210 Reward: 812.3798536886155\n","Total Timesteps: 193668 Episode Num: 211 Reward: 406.03759782640736\n","Total Timesteps: 194668 Episode Num: 212 Reward: 342.70325244838364\n","Total Timesteps: 195668 Episode Num: 213 Reward: 636.1031430507818\n","---------------------------------------\n","Average Reward over the Evaluation Step: 442.247081\n","---------------------------------------\n","Total Timesteps: 196668 Episode Num: 214 Reward: 446.5957309811432\n","Total Timesteps: 197668 Episode Num: 215 Reward: 240.56037729940155\n","Total Timesteps: 198668 Episode Num: 216 Reward: 789.2990174623487\n","Total Timesteps: 199668 Episode Num: 217 Reward: 832.841642993019\n","Total Timesteps: 200668 Episode Num: 218 Reward: 911.0902121111435\n","---------------------------------------\n","Average Reward over the Evaluation Step: 793.751146\n","---------------------------------------\n","Total Timesteps: 201668 Episode Num: 219 Reward: 851.9604240561681\n","Total Timesteps: 202668 Episode Num: 220 Reward: 574.4176980795684\n","Total Timesteps: 203668 Episode Num: 221 Reward: 739.6449628816731\n","Total Timesteps: 204668 Episode Num: 222 Reward: 678.7663217726927\n","Total Timesteps: 205668 Episode Num: 223 Reward: 767.3626807158845\n","---------------------------------------\n","Average Reward over the Evaluation Step: 861.204574\n","---------------------------------------\n","Total Timesteps: 206668 Episode Num: 224 Reward: 906.8635670669998\n","Total Timesteps: 207668 Episode Num: 225 Reward: 737.1104302703616\n","Total Timesteps: 208668 Episode Num: 226 Reward: 803.7059573379261\n","Total Timesteps: 209668 Episode Num: 227 Reward: 711.0499211269523\n","Total Timesteps: 210668 Episode Num: 228 Reward: 983.3027465722513\n","---------------------------------------\n","Average Reward over the Evaluation Step: 788.853724\n","---------------------------------------\n","Total Timesteps: 211668 Episode Num: 229 Reward: 714.2804168855483\n","Total Timesteps: 212668 Episode Num: 230 Reward: 715.9898961223874\n","Total Timesteps: 213668 Episode Num: 231 Reward: 688.5771315675028\n","Total Timesteps: 214668 Episode Num: 232 Reward: 483.07956872612505\n","Total Timesteps: 215668 Episode Num: 233 Reward: 813.0206931229039\n","---------------------------------------\n","Average Reward over the Evaluation Step: 826.966747\n","---------------------------------------\n","Total Timesteps: 216668 Episode Num: 234 Reward: 871.3209046230185\n","Total Timesteps: 217668 Episode Num: 235 Reward: 795.1211754192268\n","Total Timesteps: 218668 Episode Num: 236 Reward: 777.7931342726025\n","Total Timesteps: 219668 Episode Num: 237 Reward: 768.7825326503114\n","Total Timesteps: 220668 Episode Num: 238 Reward: 937.4944309610917\n","---------------------------------------\n","Average Reward over the Evaluation Step: 802.040445\n","---------------------------------------\n","Total Timesteps: 221668 Episode Num: 239 Reward: 782.682618006426\n","Total Timesteps: 222668 Episode Num: 240 Reward: 591.4621905078023\n","Total Timesteps: 223668 Episode Num: 241 Reward: 617.0956074630957\n","Total Timesteps: 224668 Episode Num: 242 Reward: 496.2034043283042\n","Total Timesteps: 225668 Episode Num: 243 Reward: 745.1072375597039\n","---------------------------------------\n","Average Reward over the Evaluation Step: 626.434010\n","---------------------------------------\n","Total Timesteps: 226668 Episode Num: 244 Reward: 895.5334859617078\n","Total Timesteps: 227668 Episode Num: 245 Reward: 608.9531493537771\n","Total Timesteps: 228668 Episode Num: 246 Reward: 796.1941973636737\n","Total Timesteps: 229668 Episode Num: 247 Reward: 702.5245278619377\n","Total Timesteps: 230668 Episode Num: 248 Reward: 697.1167981618491\n","---------------------------------------\n","Average Reward over the Evaluation Step: 625.291864\n","---------------------------------------\n","Total Timesteps: 231668 Episode Num: 249 Reward: 744.955360288279\n","Total Timesteps: 232668 Episode Num: 250 Reward: 561.4785220673953\n","Total Timesteps: 233668 Episode Num: 251 Reward: 670.2213227549686\n","Total Timesteps: 234668 Episode Num: 252 Reward: 799.1338194003255\n","Total Timesteps: 235668 Episode Num: 253 Reward: 1088.2431684267667\n","---------------------------------------\n","Average Reward over the Evaluation Step: 727.299138\n","---------------------------------------\n","Total Timesteps: 236668 Episode Num: 254 Reward: 759.6411984485704\n","Total Timesteps: 237668 Episode Num: 255 Reward: 721.9239551928\n","Total Timesteps: 238668 Episode Num: 256 Reward: 802.2123683490028\n","Total Timesteps: 239668 Episode Num: 257 Reward: 859.9983818481112\n","Total Timesteps: 240668 Episode Num: 258 Reward: 1197.9063019391012\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1018.987844\n","---------------------------------------\n","Total Timesteps: 241668 Episode Num: 259 Reward: 1255.9587974232209\n","Total Timesteps: 242668 Episode Num: 260 Reward: 1224.7322326994733\n","Total Timesteps: 243668 Episode Num: 261 Reward: 1284.137159579887\n","Total Timesteps: 244668 Episode Num: 262 Reward: 1301.0734493590255\n","Total Timesteps: 245668 Episode Num: 263 Reward: 1271.3824981638613\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1003.175424\n","---------------------------------------\n","Total Timesteps: 246668 Episode Num: 264 Reward: 858.1202385151075\n","Total Timesteps: 247668 Episode Num: 265 Reward: 1224.4093860611677\n","Total Timesteps: 248668 Episode Num: 266 Reward: 1197.561259856548\n","Total Timesteps: 249668 Episode Num: 267 Reward: 900.396276977393\n","Total Timesteps: 250668 Episode Num: 268 Reward: 1052.487280521295\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1229.077602\n","---------------------------------------\n","Total Timesteps: 251668 Episode Num: 269 Reward: 1141.8036839071094\n","Total Timesteps: 252668 Episode Num: 270 Reward: 1342.941597479348\n","Total Timesteps: 253668 Episode Num: 271 Reward: 1237.9510037267255\n","Total Timesteps: 254668 Episode Num: 272 Reward: 1180.6163801378773\n","Total Timesteps: 255668 Episode Num: 273 Reward: 1141.6404571864175\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1374.670338\n","---------------------------------------\n","Total Timesteps: 256668 Episode Num: 274 Reward: 1324.9435834525361\n","Total Timesteps: 257668 Episode Num: 275 Reward: 1276.4330652491722\n","Total Timesteps: 258668 Episode Num: 276 Reward: 1286.3788224617183\n","Total Timesteps: 259668 Episode Num: 277 Reward: 1232.1989925795838\n","Total Timesteps: 260668 Episode Num: 278 Reward: 1159.2919000830036\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1287.194204\n","---------------------------------------\n","Total Timesteps: 261668 Episode Num: 279 Reward: 1080.2520053321414\n","Total Timesteps: 262668 Episode Num: 280 Reward: 1266.6577500382923\n","Total Timesteps: 263668 Episode Num: 281 Reward: 1220.9862174635723\n","Total Timesteps: 264668 Episode Num: 282 Reward: 1236.80464434435\n","Total Timesteps: 265668 Episode Num: 283 Reward: 1329.6406169351574\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1457.600561\n","---------------------------------------\n","Total Timesteps: 266668 Episode Num: 284 Reward: 1453.7886700958868\n","Total Timesteps: 267668 Episode Num: 285 Reward: 1445.4809414157432\n","Total Timesteps: 268668 Episode Num: 286 Reward: 1547.7245100493003\n","Total Timesteps: 269668 Episode Num: 287 Reward: 1458.0641288231645\n","Total Timesteps: 270668 Episode Num: 288 Reward: 1357.1339730743925\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1503.432552\n","---------------------------------------\n","Total Timesteps: 271668 Episode Num: 289 Reward: 1480.4157140487878\n","Total Timesteps: 272668 Episode Num: 290 Reward: 1454.9548813166557\n","Total Timesteps: 273668 Episode Num: 291 Reward: 1384.7522466996588\n","Total Timesteps: 274668 Episode Num: 292 Reward: 1281.4823910295336\n","Total Timesteps: 275668 Episode Num: 293 Reward: 1357.4956595585961\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1378.784673\n","---------------------------------------\n","Total Timesteps: 276668 Episode Num: 294 Reward: 1291.8427612572893\n","Total Timesteps: 277668 Episode Num: 295 Reward: 1491.2342915711104\n","Total Timesteps: 278668 Episode Num: 296 Reward: 1671.2939812633092\n","Total Timesteps: 279668 Episode Num: 297 Reward: 1554.0497060780842\n","Total Timesteps: 280668 Episode Num: 298 Reward: 781.1091664660597\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1562.637076\n","---------------------------------------\n","Total Timesteps: 281668 Episode Num: 299 Reward: 1558.267589502214\n","Total Timesteps: 282668 Episode Num: 300 Reward: 1425.681346094449\n","Total Timesteps: 283668 Episode Num: 301 Reward: 1581.1821854445118\n","Total Timesteps: 284668 Episode Num: 302 Reward: 1557.0913278839546\n","Total Timesteps: 285668 Episode Num: 303 Reward: 1507.9653059979526\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1562.757564\n","---------------------------------------\n","Total Timesteps: 286668 Episode Num: 304 Reward: 1550.6344517272942\n","Total Timesteps: 287668 Episode Num: 305 Reward: 1361.519427514899\n","Total Timesteps: 288668 Episode Num: 306 Reward: 1474.2144910894287\n","Total Timesteps: 289668 Episode Num: 307 Reward: 1523.5853889230207\n","Total Timesteps: 290668 Episode Num: 308 Reward: 1422.275106610287\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1681.374501\n","---------------------------------------\n","Total Timesteps: 291668 Episode Num: 309 Reward: 1663.45730615183\n","Total Timesteps: 292668 Episode Num: 310 Reward: 1584.2191020631517\n","Total Timesteps: 293668 Episode Num: 311 Reward: 1749.6418375159205\n","Total Timesteps: 294668 Episode Num: 312 Reward: 1529.6804908968277\n","Total Timesteps: 295668 Episode Num: 313 Reward: 761.7831220745786\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1484.279380\n","---------------------------------------\n","Total Timesteps: 296668 Episode Num: 314 Reward: 1683.2976090774605\n","Total Timesteps: 297668 Episode Num: 315 Reward: 1578.3990310738093\n","Total Timesteps: 298668 Episode Num: 316 Reward: 756.2545819529704\n","Total Timesteps: 298991 Episode Num: 317 Reward: 293.05303414346565\n","Total Timesteps: 299704 Episode Num: 318 Reward: 936.7069867056892\n","Total Timesteps: 299978 Episode Num: 319 Reward: 253.50115994599872\n","Total Timesteps: 300315 Episode Num: 320 Reward: 401.6061765922738\n","---------------------------------------\n","Average Reward over the Evaluation Step: 506.445069\n","---------------------------------------\n","Total Timesteps: 301315 Episode Num: 321 Reward: 301.90607047659273\n","Total Timesteps: 302315 Episode Num: 322 Reward: 268.70741892875174\n","Total Timesteps: 303315 Episode Num: 323 Reward: 236.86696312716708\n","Total Timesteps: 304315 Episode Num: 324 Reward: 1134.0420593585825\n","Total Timesteps: 305315 Episode Num: 325 Reward: 697.3829202218742\n","---------------------------------------\n","Average Reward over the Evaluation Step: 356.080447\n","---------------------------------------\n","Total Timesteps: 306315 Episode Num: 326 Reward: 533.683783027196\n","Total Timesteps: 307315 Episode Num: 327 Reward: 302.4435874043416\n","Total Timesteps: 308315 Episode Num: 328 Reward: 486.85291775000604\n","Total Timesteps: 309315 Episode Num: 329 Reward: 411.8162341908887\n","Total Timesteps: 310315 Episode Num: 330 Reward: 538.9218026940665\n","---------------------------------------\n","Average Reward over the Evaluation Step: 493.350145\n","---------------------------------------\n","Total Timesteps: 311315 Episode Num: 331 Reward: 463.0958034523479\n","Total Timesteps: 312315 Episode Num: 332 Reward: 346.28734262612335\n","Total Timesteps: 313315 Episode Num: 333 Reward: 664.3285454256396\n","Total Timesteps: 314315 Episode Num: 334 Reward: 606.4072503067155\n","Total Timesteps: 315315 Episode Num: 335 Reward: 579.9183812001245\n","---------------------------------------\n","Average Reward over the Evaluation Step: 631.407638\n","---------------------------------------\n","Total Timesteps: 316315 Episode Num: 336 Reward: 354.03390733985646\n","Total Timesteps: 317315 Episode Num: 337 Reward: 596.7407486421097\n","Total Timesteps: 318315 Episode Num: 338 Reward: 610.7154172999714\n","Total Timesteps: 319315 Episode Num: 339 Reward: 501.64231895671736\n","Total Timesteps: 320315 Episode Num: 340 Reward: 732.836796962342\n","---------------------------------------\n","Average Reward over the Evaluation Step: 444.727555\n","---------------------------------------\n","Total Timesteps: 321315 Episode Num: 341 Reward: 422.7578662180926\n","Total Timesteps: 322315 Episode Num: 342 Reward: 440.9876473762758\n","Total Timesteps: 323315 Episode Num: 343 Reward: 481.26171138317557\n","Total Timesteps: 324315 Episode Num: 344 Reward: 406.10956733538967\n","Total Timesteps: 325315 Episode Num: 345 Reward: 444.4250816532839\n","---------------------------------------\n","Average Reward over the Evaluation Step: 392.312825\n","---------------------------------------\n","Total Timesteps: 326315 Episode Num: 346 Reward: 418.3013749800879\n","Total Timesteps: 327315 Episode Num: 347 Reward: 503.89027625738794\n","Total Timesteps: 328315 Episode Num: 348 Reward: 367.4655489600044\n","Total Timesteps: 329315 Episode Num: 349 Reward: 266.0343530961277\n","Total Timesteps: 330315 Episode Num: 350 Reward: 424.2052446634074\n","---------------------------------------\n","Average Reward over the Evaluation Step: 243.559258\n","---------------------------------------\n","Total Timesteps: 331315 Episode Num: 351 Reward: 323.0216093545203\n","Total Timesteps: 331522 Episode Num: 352 Reward: 150.17504211970513\n","Total Timesteps: 332522 Episode Num: 353 Reward: 358.55497710547917\n","Total Timesteps: 332790 Episode Num: 354 Reward: 184.69304801042892\n","Total Timesteps: 333152 Episode Num: 355 Reward: 190.7650379527851\n","Total Timesteps: 334152 Episode Num: 356 Reward: 366.21279632639823\n","Total Timesteps: 334790 Episode Num: 357 Reward: 410.0242057485193\n","Total Timesteps: 335790 Episode Num: 358 Reward: 302.5708864614644\n","---------------------------------------\n","Average Reward over the Evaluation Step: 325.192330\n","---------------------------------------\n","Total Timesteps: 336790 Episode Num: 359 Reward: 320.71674674246805\n","Total Timesteps: 337070 Episode Num: 360 Reward: 218.56722417329203\n","Total Timesteps: 338070 Episode Num: 361 Reward: 217.7035138956646\n","Total Timesteps: 338331 Episode Num: 362 Reward: 191.50293938725395\n","Total Timesteps: 338576 Episode Num: 363 Reward: 185.4683004259639\n","Total Timesteps: 338820 Episode Num: 364 Reward: 230.45505148375912\n","Total Timesteps: 339820 Episode Num: 365 Reward: 212.37753629038147\n","Total Timesteps: 340820 Episode Num: 366 Reward: 432.97972710989325\n","---------------------------------------\n","Average Reward over the Evaluation Step: 396.622974\n","---------------------------------------\n","Total Timesteps: 341820 Episode Num: 367 Reward: 356.47843448561997\n","Total Timesteps: 342820 Episode Num: 368 Reward: 474.33278345551656\n","Total Timesteps: 343820 Episode Num: 369 Reward: 333.36056714399297\n","Total Timesteps: 344820 Episode Num: 370 Reward: 449.3226373168949\n","Total Timesteps: 345820 Episode Num: 371 Reward: 542.2171246568015\n","---------------------------------------\n","Average Reward over the Evaluation Step: 632.706457\n","---------------------------------------\n","Total Timesteps: 346820 Episode Num: 372 Reward: 384.0725709675919\n","Total Timesteps: 347820 Episode Num: 373 Reward: 532.0946351074306\n","Total Timesteps: 348820 Episode Num: 374 Reward: 489.37296797962296\n","Total Timesteps: 349820 Episode Num: 375 Reward: 538.1874299360554\n","Total Timesteps: 350820 Episode Num: 376 Reward: 602.2328206699849\n","---------------------------------------\n","Average Reward over the Evaluation Step: 571.678691\n","---------------------------------------\n","Total Timesteps: 351820 Episode Num: 377 Reward: 605.4929864927324\n","Total Timesteps: 352820 Episode Num: 378 Reward: 585.4520280576041\n","Total Timesteps: 353820 Episode Num: 379 Reward: 594.1730306213324\n","Total Timesteps: 354820 Episode Num: 380 Reward: 701.2493418076563\n","Total Timesteps: 355820 Episode Num: 381 Reward: 617.7558666572955\n","---------------------------------------\n","Average Reward over the Evaluation Step: 669.505481\n","---------------------------------------\n","Total Timesteps: 356820 Episode Num: 382 Reward: 638.3069935305911\n","Total Timesteps: 357820 Episode Num: 383 Reward: 752.3721243279923\n","Total Timesteps: 358820 Episode Num: 384 Reward: 790.3474934887934\n","Total Timesteps: 359820 Episode Num: 385 Reward: 650.9803714705948\n","Total Timesteps: 360820 Episode Num: 386 Reward: 484.7842391416296\n","---------------------------------------\n","Average Reward over the Evaluation Step: 620.807458\n","---------------------------------------\n","Total Timesteps: 361820 Episode Num: 387 Reward: 637.030225069501\n","Total Timesteps: 362820 Episode Num: 388 Reward: 615.6858046573396\n","Total Timesteps: 363820 Episode Num: 389 Reward: 593.8068126852517\n","Total Timesteps: 364820 Episode Num: 390 Reward: 511.2383791048537\n","Total Timesteps: 365820 Episode Num: 391 Reward: 537.4005687350189\n","---------------------------------------\n","Average Reward over the Evaluation Step: 472.098751\n","---------------------------------------\n","Total Timesteps: 366820 Episode Num: 392 Reward: 469.6030410619073\n","Total Timesteps: 367820 Episode Num: 393 Reward: 631.0200010367635\n","Total Timesteps: 368820 Episode Num: 394 Reward: 666.3160185925195\n","Total Timesteps: 369820 Episode Num: 395 Reward: 645.3049381744815\n","Total Timesteps: 370820 Episode Num: 396 Reward: 1214.426775989506\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1409.884375\n","---------------------------------------\n","Total Timesteps: 371820 Episode Num: 397 Reward: 1418.668442025864\n","Total Timesteps: 372820 Episode Num: 398 Reward: 1255.140440388606\n","Total Timesteps: 373820 Episode Num: 399 Reward: 1400.979211559418\n","Total Timesteps: 374820 Episode Num: 400 Reward: 1339.3964990922846\n","Total Timesteps: 375820 Episode Num: 401 Reward: 1279.9261589139671\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1484.876219\n","---------------------------------------\n","Total Timesteps: 376820 Episode Num: 402 Reward: 1490.5521265916545\n","Total Timesteps: 377820 Episode Num: 403 Reward: 1554.8406195438426\n","Total Timesteps: 378820 Episode Num: 404 Reward: 1608.1790114396224\n","Total Timesteps: 379820 Episode Num: 405 Reward: 1493.4371742908124\n","Total Timesteps: 380820 Episode Num: 406 Reward: 1674.9943414173424\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1624.497891\n","---------------------------------------\n","Total Timesteps: 381820 Episode Num: 407 Reward: 1640.6026307392663\n","Total Timesteps: 382820 Episode Num: 408 Reward: 1672.7852374396202\n","Total Timesteps: 383820 Episode Num: 409 Reward: 1520.9546873729273\n","Total Timesteps: 384820 Episode Num: 410 Reward: 1445.5349432453224\n","Total Timesteps: 385820 Episode Num: 411 Reward: 1786.7482351277179\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1834.502103\n","---------------------------------------\n","Total Timesteps: 386820 Episode Num: 412 Reward: 1796.5503273673717\n","Total Timesteps: 387820 Episode Num: 413 Reward: 1496.243763504117\n","Total Timesteps: 388820 Episode Num: 414 Reward: 1515.7542750482212\n","Total Timesteps: 389820 Episode Num: 415 Reward: 997.9171789060596\n","Total Timesteps: 390820 Episode Num: 416 Reward: 1367.8014219466181\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1603.337418\n","---------------------------------------\n","Total Timesteps: 391820 Episode Num: 417 Reward: 1605.1803724659324\n","Total Timesteps: 392820 Episode Num: 418 Reward: 1655.896817520554\n","Total Timesteps: 393820 Episode Num: 419 Reward: 1876.4927523376814\n","Total Timesteps: 394820 Episode Num: 420 Reward: 1623.5295822936173\n","Total Timesteps: 395820 Episode Num: 421 Reward: 1881.7502070948483\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1801.165355\n","---------------------------------------\n","Total Timesteps: 396820 Episode Num: 422 Reward: 1831.7726275453995\n","Total Timesteps: 397820 Episode Num: 423 Reward: 1640.062754476134\n","Total Timesteps: 398820 Episode Num: 424 Reward: 1618.3623747281715\n","Total Timesteps: 399820 Episode Num: 425 Reward: 1757.7844982952995\n","Total Timesteps: 400820 Episode Num: 426 Reward: 1747.8763289855306\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1947.574943\n","---------------------------------------\n","Total Timesteps: 401820 Episode Num: 427 Reward: 1892.260200912323\n","Total Timesteps: 402820 Episode Num: 428 Reward: 1801.4847985811716\n","Total Timesteps: 403820 Episode Num: 429 Reward: 1858.6175963012013\n","Total Timesteps: 404820 Episode Num: 430 Reward: 1801.5507458238026\n","Total Timesteps: 405820 Episode Num: 431 Reward: 1829.8739702440507\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1850.702524\n","---------------------------------------\n","Total Timesteps: 406820 Episode Num: 432 Reward: 1793.650284746774\n","Total Timesteps: 407820 Episode Num: 433 Reward: 1763.3033480292079\n","Total Timesteps: 408820 Episode Num: 434 Reward: 1871.9973906714201\n","Total Timesteps: 409820 Episode Num: 435 Reward: 1950.8206413058424\n","Total Timesteps: 410820 Episode Num: 436 Reward: 1828.1344634508011\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1722.194764\n","---------------------------------------\n","Total Timesteps: 411820 Episode Num: 437 Reward: 1723.4213181234786\n","Total Timesteps: 412820 Episode Num: 438 Reward: 1887.1892923708485\n","Total Timesteps: 413820 Episode Num: 439 Reward: 1681.2862741383979\n","Total Timesteps: 414820 Episode Num: 440 Reward: 1903.311933067289\n","Total Timesteps: 415820 Episode Num: 441 Reward: 1879.511521069455\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1881.397888\n","---------------------------------------\n","Total Timesteps: 416820 Episode Num: 442 Reward: 1850.6977105453234\n","Total Timesteps: 417820 Episode Num: 443 Reward: 1705.7780790447346\n","Total Timesteps: 418820 Episode Num: 444 Reward: 1854.4775992576233\n","Total Timesteps: 419820 Episode Num: 445 Reward: 1776.972139869928\n","Total Timesteps: 420820 Episode Num: 446 Reward: 1731.4901483590402\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1701.213124\n","---------------------------------------\n","Total Timesteps: 421820 Episode Num: 447 Reward: 1711.263449346245\n","Total Timesteps: 422820 Episode Num: 448 Reward: 1653.4137159372792\n","Total Timesteps: 423820 Episode Num: 449 Reward: 1820.5887478471207\n","Total Timesteps: 424820 Episode Num: 450 Reward: 1749.2375368187404\n","Total Timesteps: 425820 Episode Num: 451 Reward: 1857.28623969246\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1811.331594\n","---------------------------------------\n","Total Timesteps: 426820 Episode Num: 452 Reward: 1779.0250876517243\n","Total Timesteps: 427820 Episode Num: 453 Reward: 1701.808899301459\n","Total Timesteps: 428820 Episode Num: 454 Reward: 1816.6810826827489\n","Total Timesteps: 429820 Episode Num: 455 Reward: 1827.389436622388\n","Total Timesteps: 430820 Episode Num: 456 Reward: 2027.4078961113635\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2023.424088\n","---------------------------------------\n","Total Timesteps: 431820 Episode Num: 457 Reward: 1978.7954160324916\n","Total Timesteps: 432820 Episode Num: 458 Reward: 1941.054990185844\n","Total Timesteps: 433820 Episode Num: 459 Reward: 1833.4818366435632\n","Total Timesteps: 434820 Episode Num: 460 Reward: 1981.7542674779431\n","Total Timesteps: 435820 Episode Num: 461 Reward: 2124.220743970367\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2039.098950\n","---------------------------------------\n","Total Timesteps: 436820 Episode Num: 462 Reward: 1946.2451315926182\n","Total Timesteps: 437820 Episode Num: 463 Reward: 1992.2995291124698\n","Total Timesteps: 438820 Episode Num: 464 Reward: 2064.4084484313767\n","Total Timesteps: 439820 Episode Num: 465 Reward: 1997.5458572381333\n","Total Timesteps: 440820 Episode Num: 466 Reward: 2257.5646930851854\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1991.466610\n","---------------------------------------\n","Total Timesteps: 441820 Episode Num: 467 Reward: 2010.4890931564862\n","Total Timesteps: 442820 Episode Num: 468 Reward: 1922.1837072140738\n","Total Timesteps: 443820 Episode Num: 469 Reward: 1962.165073643868\n","Total Timesteps: 444820 Episode Num: 470 Reward: 1870.1764855850479\n","Total Timesteps: 445820 Episode Num: 471 Reward: 1956.7221345153978\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1933.236423\n","---------------------------------------\n","Total Timesteps: 446820 Episode Num: 472 Reward: 1918.9470585606764\n","Total Timesteps: 447820 Episode Num: 473 Reward: 1857.8431307022452\n","Total Timesteps: 448820 Episode Num: 474 Reward: 2098.551717648711\n","Total Timesteps: 449820 Episode Num: 475 Reward: 2000.9369744255503\n","Total Timesteps: 450820 Episode Num: 476 Reward: 2153.4044519285285\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2075.934402\n","---------------------------------------\n","Total Timesteps: 451820 Episode Num: 477 Reward: 2039.9828121394025\n","Total Timesteps: 452820 Episode Num: 478 Reward: 2046.2012310438727\n","Total Timesteps: 453820 Episode Num: 479 Reward: 2081.13417091557\n","Total Timesteps: 454820 Episode Num: 480 Reward: 2041.634954691534\n","Total Timesteps: 455820 Episode Num: 481 Reward: 2063.2522871094816\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2121.104090\n","---------------------------------------\n","Total Timesteps: 456820 Episode Num: 482 Reward: 2082.974259475054\n","Total Timesteps: 457820 Episode Num: 483 Reward: 1946.0437567569788\n","Total Timesteps: 458820 Episode Num: 484 Reward: 1880.235910268855\n","Total Timesteps: 459820 Episode Num: 485 Reward: 1949.7436775098713\n","Total Timesteps: 460820 Episode Num: 486 Reward: 2008.1378559926638\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2075.118391\n","---------------------------------------\n","Total Timesteps: 461820 Episode Num: 487 Reward: 2022.2976986831486\n","Total Timesteps: 462820 Episode Num: 488 Reward: 2135.6725803889076\n","Total Timesteps: 463820 Episode Num: 489 Reward: 2085.530700212352\n","Total Timesteps: 464820 Episode Num: 490 Reward: 2057.815625672856\n","Total Timesteps: 465820 Episode Num: 491 Reward: 2165.7619959235444\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2203.217990\n","---------------------------------------\n","Total Timesteps: 466820 Episode Num: 492 Reward: 2168.2888284471123\n","Total Timesteps: 467820 Episode Num: 493 Reward: 2217.0921531759777\n","Total Timesteps: 468820 Episode Num: 494 Reward: 2119.8582693616904\n","Total Timesteps: 469820 Episode Num: 495 Reward: 2094.1747569309327\n","Total Timesteps: 470820 Episode Num: 496 Reward: 2130.0374550297065\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2086.987208\n","---------------------------------------\n","Total Timesteps: 471820 Episode Num: 497 Reward: 2080.1052167651883\n","Total Timesteps: 472820 Episode Num: 498 Reward: 2143.696490030027\n","Total Timesteps: 473820 Episode Num: 499 Reward: 2097.456497327816\n","Total Timesteps: 474820 Episode Num: 500 Reward: 2080.0979529565093\n","Total Timesteps: 475820 Episode Num: 501 Reward: 2105.8547140606584\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2227.145461\n","---------------------------------------\n","Total Timesteps: 476820 Episode Num: 502 Reward: 2194.054842722061\n","Total Timesteps: 477820 Episode Num: 503 Reward: 2105.5612717833087\n","Total Timesteps: 478820 Episode Num: 504 Reward: 2101.500553934185\n","Total Timesteps: 479820 Episode Num: 505 Reward: 2127.3424903456475\n","Total Timesteps: 480820 Episode Num: 506 Reward: 2048.235528087713\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2182.207470\n","---------------------------------------\n","Total Timesteps: 481820 Episode Num: 507 Reward: 2159.9762449769805\n","Total Timesteps: 482820 Episode Num: 508 Reward: 2114.7522623972577\n","Total Timesteps: 483820 Episode Num: 509 Reward: 2182.8186412543614\n","Total Timesteps: 484820 Episode Num: 510 Reward: 2189.8013500662214\n","Total Timesteps: 485820 Episode Num: 511 Reward: 2285.085867920048\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2289.485222\n","---------------------------------------\n","Total Timesteps: 486820 Episode Num: 512 Reward: 2313.8915186085383\n","Total Timesteps: 487820 Episode Num: 513 Reward: 2231.3747780866106\n","Total Timesteps: 488820 Episode Num: 514 Reward: 2252.852706470454\n","Total Timesteps: 489820 Episode Num: 515 Reward: 2296.1784069923515\n","Total Timesteps: 490820 Episode Num: 516 Reward: 2211.2772390482064\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2079.973130\n","---------------------------------------\n","Total Timesteps: 491820 Episode Num: 517 Reward: 2069.58908770483\n","Total Timesteps: 492820 Episode Num: 518 Reward: 2297.434953309129\n","Total Timesteps: 493820 Episode Num: 519 Reward: 2281.6379569659102\n","Total Timesteps: 494820 Episode Num: 520 Reward: 2312.2501881289118\n","Total Timesteps: 495820 Episode Num: 521 Reward: 2213.4097837484906\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2159.306068\n","---------------------------------------\n","Total Timesteps: 496820 Episode Num: 522 Reward: 2161.8179408779692\n","Total Timesteps: 497820 Episode Num: 523 Reward: 2071.6180458308254\n","Total Timesteps: 498820 Episode Num: 524 Reward: 2282.0530950677553\n","Total Timesteps: 499820 Episode Num: 525 Reward: 2139.05370501728\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2176.346005\n","---------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wi6e2-_pu05e","colab_type":"text"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"oW4d1YAMqif1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1592495699993,"user_tz":-120,"elapsed":98193,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"outputId":"069cd44f-0ab7-4452-bb75-87f2fc4dee76"},"source":["class Actor(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x)) \n","    return x\n","\n","class Critic(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1\n","\n","# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","    \n","    for it in range(iterations):\n","      \n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","      \n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","      \n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","      \n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","      \n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","      \n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","      \n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","      \n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","      \n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","      \n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","        \n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","        \n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","  \n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","  \n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n","\n","def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward\n","\n","env_name = \"AntBulletEnv-v0\"\n","seed = 0\n","\n","file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")\n","\n","eval_episodes = 10\n","save_env_vid = True\n","env = gym.make(env_name)\n","max_episode_steps = env._max_episode_steps\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()\n","env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])\n","policy = TD3(state_dim, action_dim, max_action)\n","policy.load(file_name, './pytorch_models/')\n","_ = evaluate_policy(policy, eval_episodes=eval_episodes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------------\n","Settings: TD3_AntBulletEnv-v0_0\n","---------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["---------------------------------------\n","Average Reward over the Evaluation Step: 2181.065678\n","---------------------------------------\n"],"name":"stdout"}]}]}